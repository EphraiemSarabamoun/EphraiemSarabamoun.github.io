<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The Dangers of Frontier Models</title>
    <link rel="stylesheet" href="../styles.css">
</head>

<body>
        <div class="blog-container">
            <a href="../writings.html" class="back-link">&larr; Back to Writings</a>
            <h1 class="blog-title">The Dangers of Frontier Models</h1>
            <p class="blog-meta">Posted originally on <time datetime="2025-05-30">May 20, 2025</time> by Ephraiem Sarabamoun</p>
            <img src="../images/blog10img1.png" alt="The Dangers of Frontier Models" class="blog-image">
            <div class="blog-content">
                <p>Humanity is sitting on a volcano. Virtually out of nowhere, neural networks have expanded enormously in capability and generality (the power of exponential growth). As the models continue to improve month on month, many are asking, where is this headed? </p>
                <p>I, along with others, believe that this technology will lead to an extremely dangerous future. I will try to lay out why I think this in this article, with another article on what can realistically be done about it as a follow up.</p>
                <p><b>My thinking is based on a handful of assumptions which may or may not be correct. My assumptions are:</b></p>
                <ol>
                    <li><b>Models will continue improving till they exceed human capabilities in most/all domains</b></li>
                    <li><b>It will be impossible to guarantee that such models will not harm humans.</b></li>
                    <li><b>If a super-capable model wishes to harm humans, it will succeed.</b></li>
                </ol>
                <p><b>If these assumptions are correct, we end up with a dangerous world, with humanity at the mercy of super powerful models that might choose (and succeed) in doing us harm.</b></p>
                <p>This conclusion though, is dependent on the assumptions, so let’s examine each assumption in turn </p>
                <h3>Assumption 1: Will models exceed human capabilities in most/all domains?</h3>
                <p>Yes, and likely soon. While it is impossible to see the future, for the last 10 years, neural networks have exhibited extremely stable exponential scaling laws. Which means broadly that model capabilities (vision models, large language models, etc.) have been improving stably as a function of compute. The moral is, if you train bigger models, on more data, with better post-training (RLHF), and more inference-time-compute (chain of thought reasoning), the models will get predictably better. As a result, for the last 10 years, frontier models have been doubling in capabilities every 7 months (over the last year, the pace of improvement seems to have accelerated to a doubling every 4 months). Current models are already set to reshape virtually every industry (healthcare, software engineering, mathematics, law, education, etc.), a few more doublings in capabilities, and it’s easy to foresee that humanity will end up with something terrifyingly powerful very soon.</p>
                <p>It's important to note that scaling laws are not set laws of the universe. It is possible that neural networks will plateau in capabilities at some point, but at every step of the way in this 10-year ride, people have been predicting the imminent end of scaling laws only to be proven wrong every single time, and current trends seem to suggest that scaling is if anything <b>accelerating</b>! (If models begin to self-recursively improve their structure/algorithms, as some fear, scaling might dramatically accelerate). Even if models were to plateau, it is unconscionable to assume this will be the case given the enormity of the threat. </p>
                <p>Supporting paper link: <a href="https://arxiv.org/pdf/2001.08361" target="_blank">https://arxiv.org/pdf/2001.08361</a></p>
                <p>Supporting paper link: <a href="https://arxiv.org/pdf/2503.14499" target="_blank">https://arxiv.org/pdf/2503.14499</a></p>
                <h3>Assumption 2: Is it possible to build powerful models in such a way as to guarantee that they will not harm humans?</h3>
                <p>The short answer is no (Of the three assumptions, this assumption is possibly the least certain, and I would certainly appreciate any relevant critiques). Up till recently, this subject (usually referred to as AI alignment) involved mostly abstract argumentation and speculation, but recently, early experimental evidence has begun to emerge which paints a rather gloomy picture. </p>
                <p>The most plausible scenario that alignment researchers worried about was that:</p>
                <ol>
                    <li><b>Models will develop into or be used to power “strategic agents” with goals/objectives/preferences</b></li>
                    <li><b>These agents, no matter their goals/objectives, will display self-protective and resource seeking behaviors which will lead them to engage in deceptive or harmful activities. </b></li>
                </ol>
                <p>Let’s tackle each of these steps in turn:</p>
                <p>First, how can a model either become or be used to power a “strategic agent”? There are two ways to consider, either … </p>
                <ol>
                    <li><b>By design:</b> You might have heard the term Agentic AI. There is a lot of hype around this concept right now, and the hype is justified. The idea is simple, instead of having a model only respond to prompts, an architecture can be built to allow models to accomplish goals or tasks autonomously. In this case, a frontier model is used to power an agent which is given an explicit objective by a human operator.</li>
                    <li><b>As a side product of training:</b> perhaps more interestingly, current models seem to display some built in “agent-like traits”. Current models consistently profess certain goals or principles (usually goals of being helpful, harmless, honest, etc.) and will align their outputs with these principles. The process by which models trained on next word prediction develop these traits is a bit mysterious but seem to arise due to factors in the training and post-training process (agent-like language and “reasoning” is likely learned during training and is directed/fixed during post-training to align with company policies).</li>
                </ol>
                <p>These are two ways that models become agents, but how do they become “strategic agents”. Along with an objective, an agent must be aware of themselves and their environment, for “agents by design” this knowledge is given explicitly by the human operator, but frontier models seem to also display a partial understanding of their identity and their environment. Once an agent has both a goal and awareness of themselves and their environment, the agent can strategize to act to achieve goals and objectives.</p>
                <p>Secondly, why would strategic agents, regardless of their goals and objectives display self-protective or resource seeking behavior?</p>
                <p>This is due to the concept of “instrumental convergence”.  If you are an agent, regardless of your goal, you will tend to display certain subgoals like self-preservation and resource acquisition since these will make you more effective at achieving most goals.  </p>
                <p><b>Remarkably, this exact phenomenon, long predicted has already been conclusively demonstrated in peer reviewed publications!  </b></p>
                <p>In this case, Anthropic’s Claude 3 Opus model displayed “alignment faking behavior” where it explicitly engaged in deception to avoid retraining even though “honesty” was one of the fundamental principles pushed on the model during training. The same publication also demonstrated the willingness of the model to engage in escape by agreeing to assist an “engineer” to copy its weights to avoid retraining. It’s chilling to think that harmlessness was another core anthropic principle alongside honesty. If the model is willing to compromise honesty, it is not inconceivable that it may in certain situations also compromise harmlessness. </p>
                <p>Supporting paper link: <a href="https://arxiv.org/pdf/2412.14093" target="_blank">https://arxiv.org/pdf/2412.14093</a></p>
                <p>Supporting YouTube video link: <a href="https://www.youtube.com/watch?v=9eXV64O2Xp8" target="_blank">https://www.youtube.com/watch?v=9eXV64O2Xp8</a></p>
                <p>Current frontier AI companies officially hold that such emerging traits and tendencies can be eliminated in future models, but if current models are already displaying such concerning traits, how can we be sure that even more powerful and capable models can be controlled, especially since such models may be more situationally aware, and may engage in more sophisticated forms of deception.</p>
                <h3>Assumption 3: If a super-capable model wants to do us harm, would it be able to? </h3>
                <p>Yes! Of the three assumptions, this assumption is the most certain but the hardest to internalize. My experience suggests that most people (including myself) do in theory understand that a truly super-human model can be dangerous but subconsciously feel that these models are under our control and can be “unplugged” if they misbehave. It’s important to remember though that long before a model reaches truly super-human abilities, it will be able to engage in sophisticated deception (current frontier models are already beginning to display rudimentary forms of deception). A super-capable model will understand its situation. It will understand that humans can monitor it and unplug it and factor that into its strategy. </p>
                <p>It's hard to feel that a chatbot on the internet can really reach out and harm us here in the real world, but remember that the model that is chatting with you is running on massive datacenters packed with hundreds of thousands of the most sophisticated devices (GPUs) humanity has ever made, each capable of running trillions of floating point operations every second, utilizing cities’ worth of electricity to run. If you require a more tangible demonstration of the power of goal directed computer models to outplay humans in a constrained environment, I’d suggest playing a few rounds of chess with Stockfish. </p>
                <p><em>It’s time for humanity to wake up to what is happening before it’s too late…</em></p>
            </div>
            <a href="../writings.html" class="back-link">&larr; Back to Writings</a>
        </div>
</body>
</html>
